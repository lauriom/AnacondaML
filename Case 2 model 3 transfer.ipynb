{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5e63c1",
   "metadata": {},
   "source": [
    "# Case 2 - Tranfer model\n",
    "Neural Networks for Machine Learning Applications<br>\n",
    "24.2.2022, Jesse Jyrälä & Lauri Marjanen<br>\n",
    "Metropolia University of Applied Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3495a",
   "metadata": {},
   "source": [
    "# Idea of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460c1f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026fabec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.7.0\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "print('Tensorflow version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ff0c4",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a7396",
   "metadata": {},
   "source": [
    "Fetching data and splitting it into training and validation data. 4 to 1 split seems to work best but 10 to 1 could also be used with good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b163ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fdf42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORMAL', 'PNEUMONIA']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"D:/Koulu/Datasets/Case2Data/chest_xray/train\"\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395b5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n",
      "Using 4173 files for training.\n",
      "Found 5216 files belonging to 2 classes.\n",
      "Using 1043 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets for training and validation with 4 to 1 split\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split = 0.2,\n",
    "  subset = \"training\",\n",
    "  seed = 123,\n",
    "  image_size = IMG_SIZE,\n",
    "  batch_size = BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split = 0.2,\n",
    "  subset = \"validation\",\n",
    "  seed = 123,\n",
    "  image_size = IMG_SIZE,\n",
    "  batch_size = BATCH_SIZE)\n",
    "\n",
    "IMG_SHAPE = IMG_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c13a12",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ba4eb",
   "metadata": {},
   "source": [
    "First using keras pretarained models to base train the model. Testing ResNet50 and ResNet101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693d846",
   "metadata": {},
   "source": [
    "!!Remeber to comment out all pretrained models but the one you want to use!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9621cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "171450368/171446536 [==============================] - 5s 0us/step\n",
      "171458560/171446536 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#pretrained model with ResNet101\n",
    "\n",
    "preprocess_input = tf.keras.applications.resnet.preprocess_input\n",
    "\n",
    "base_model = tf.keras.applications.ResNet101(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c661c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained model with ResNet50\n",
    "\n",
    "# preprocess_input = tf.keras.applications.resnet.preprocess_input\n",
    "\n",
    "# base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "\n",
    "# base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb79829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting layers from keras for the child model to use\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b589d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling the child\n",
    "\n",
    "inputs = tf.keras.Input(shape = IMG_SHAPE)\n",
    "holder = preprocess_input(inputs)\n",
    "holder = base_model(holder, training = False)\n",
    "holder = global_average_layer(holder)\n",
    "holder = tf.keras.layers.Dropout(0.2)(holder)\n",
    "outputs = prediction_layer(holder)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e204a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling model. EDIT OUT: ei mitään hajuu mitä toi leraning rate tekee\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6140f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeacedd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 41s 1s/step - loss: 0.8915 - accuracy: 0.4295\n",
      "Validation loss:     0.89\n",
      "Validation accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "loss1, accuracy1 = model.evaluate(val_dataset)\n",
    "\n",
    "print(f\"Validation loss:     {loss1:.2f}\")\n",
    "print(f\"Validation accuracy: {accuracy1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef41c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "131/131 [==============================] - 158s 1s/step - loss: 0.5944 - accuracy: 0.7103\n",
      "Epoch 2/5\n",
      "131/131 [==============================] - 155s 1s/step - loss: 0.3004 - accuracy: 0.8567\n",
      "Epoch 3/5\n",
      "131/131 [==============================] - 153s 1s/step - loss: 0.2332 - accuracy: 0.8994\n",
      "Epoch 4/5\n",
      "131/131 [==============================] - 154s 1s/step - loss: 0.2050 - accuracy: 0.9101\n",
      "Epoch 5/5\n",
      "131/131 [==============================] - 155s 1s/step - loss: 0.1877 - accuracy: 0.9207\n"
     ]
    }
   ],
   "source": [
    "trained = model.fit(train_dataset, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132f492",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0b9d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 40s 1s/step - loss: 0.1199 - accuracy: 0.9521\n",
      "Validation loss:     0.12\n",
      "Validation accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "loss1, accuracy1 = model.evaluate(val_dataset)\n",
    "\n",
    "print(f\"Validation loss:     {loss1:.2f}\")\n",
    "print(f\"Validation accuracy: {accuracy1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfe84ae0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_30244/1403157185.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0macc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrained\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'accuracy'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mval_acc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrained\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'val_accuracy'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrained\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'loss'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mval_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrained\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'val_loss'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "\n",
    "acc = trained.history['accuracy']\n",
    "val_acc = trained.history['val_accuracy']\n",
    "\n",
    "loss = trained.history['loss']\n",
    "val_loss = trained.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36110948",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60744317",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}