{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5e63c1",
   "metadata": {},
   "source": [
    "# Case 2 - Transfer model\n",
    "Neural Networks for Machine Learning Applications<br>\n",
    "24.2.2022, Jesse Jyrälä & Lauri Marjanen<br>\n",
    "Metropolia University of Applied Sciences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3495a",
   "metadata": {},
   "source": [
    "# Idea of this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460c1f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026fabec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.8.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "# Ensure that installs for GPU running exist\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ff0c4",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a7396",
   "metadata": {},
   "source": [
    "Fetching data and splitting it into training and validation data. 4 to 1 split seems to work best but 10 to 1 could also be used with good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b163ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fdf42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORMAL', 'PNEUMONIA']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curDir = os.getcwd()\n",
    "data_dir = curDir + '/Case2Data/chest_xray/train'\n",
    "\n",
    "# data_dir = \"D:/Koulu/Datasets/Case2Data/chest_xray/train\"\n",
    "# data_test_dir = \"D:/Koulu/Datasets/Case2Data/chest_xray/test\"\n",
    "# data_comb_dir = \"D:/Koulu/Datasets/Case2Data/chest_xray/CombinedTestTrain\"\n",
    "# val_data_dir = \"D:/Koulu/Datasets/Case2Data/chest_xray/val\"\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395b5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n",
      "Using 4173 files for training.\n",
      "Found 5216 files belonging to 2 classes.\n",
      "Using 1043 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets for training and validation with 4 to 1 split\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split = 0.2,\n",
    "  subset = \"training\",\n",
    "  seed = 123,\n",
    "  image_size = IMG_SIZE,\n",
    "  batch_size = BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split = 0.2,\n",
    "  subset = \"validation\",\n",
    "  seed = 123,\n",
    "  image_size = IMG_SIZE,\n",
    "  batch_size = BATCH_SIZE)\n",
    "\n",
    "IMG_SHAPE = IMG_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e9f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating datasets for training and validation with 4 to 1 split\n",
    "# dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#   data_dir,\n",
    "#   image_size = IMG_SIZE,\n",
    "#   batch_size = BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c53b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9560a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c13a12",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ba4eb",
   "metadata": {},
   "source": [
    "First using keras pretarained models to base train the model. Testing ResNet50 and ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9124946",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693d846",
   "metadata": {},
   "source": [
    "!!Remeber to comment out all pretrained models but the one you want to use!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9621cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained model with InceptionV3\n",
    "\n",
    "# preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "# base_model = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "\n",
    "# base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a504bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pretrained model with InceptionResNet_V2\n",
    "\n",
    "# preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
    "\n",
    "# base_model = tf.keras.applications.InceptionResNetV2(input_shape=IMG_SHAPE,\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "\n",
    "# base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c661c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pretrained model with ResNet50\n",
    "\n",
    "preprocess_input = tf.keras.applications.resnet.preprocess_input\n",
    "\n",
    "base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df1730e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "171450368/171446536 [==============================] - 6s 0us/step\n",
      "171458560/171446536 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#pretrained model with ResNet101\n",
    "\n",
    "# preprocess_input = tf.keras.applications.resnet.preprocess_input\n",
    "\n",
    "# base_model = tf.keras.applications.ResNet101(input_shape=IMG_SHAPE,\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "\n",
    "# base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0d69ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained model with VGG16 or VGG19\n",
    "\n",
    "# preprocess_input = tf.keras.applications.vgg19.preprocess_input\n",
    "\n",
    "# base_model = tf.keras.applications.VGG19(input_shape=IMG_SHAPE,\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "\n",
    "# base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb79829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting layers from keras for the child model to use\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b589d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling the child\n",
    "\n",
    "inputs = tf.keras.Input(shape = IMG_SHAPE)\n",
    "holder = preprocess_input(inputs)\n",
    "holder = base_model(holder, training = False)\n",
    "holder = global_average_layer(holder)\n",
    "holder = tf.keras.layers.Dropout(0.2)(holder)\n",
    "outputs = prediction_layer(holder)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e204a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling model. EDIT OUT: ei mitään hajuu mitä toi leraning rate tekee\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a57ee780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 128, 128, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 128, 128, 3)      0         \n",
      "                                                                 \n",
      " resnet101 (Functional)      (None, 4, 4, 2048)        42658176  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,660,225\n",
      "Trainable params: 2,049\n",
      "Non-trainable params: 42,658,176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47d19462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(2048, 1) dtype=float32, numpy=\n",
       " array([[-0.01777567],\n",
       "        [ 0.00167895],\n",
       "        [ 0.01812768],\n",
       "        ...,\n",
       "        [-0.04235227],\n",
       "        [-0.02008777],\n",
       "        [-0.00365984]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6140f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeacedd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 47s 1s/step - loss: 0.6961 - accuracy: 0.4554\n",
      "Validation loss:     0.70\n",
      "Validation accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "loss1, accuracy1 = model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Validation loss:     {loss1:.2f}\")\n",
    "print(f\"Validation accuracy: {accuracy1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef41c2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "131/131 [==============================] - 191s 1s/step - loss: 0.4208 - accuracy: 0.7973\n",
      "Epoch 2/5\n",
      "131/131 [==============================] - 186s 1s/step - loss: 0.2534 - accuracy: 0.8893\n",
      "Epoch 3/5\n",
      "131/131 [==============================] - 187s 1s/step - loss: 0.2157 - accuracy: 0.9089\n",
      "Epoch 4/5\n",
      "131/131 [==============================] - 185s 1s/step - loss: 0.1891 - accuracy: 0.9185\n",
      "Epoch 5/5\n",
      "131/131 [==============================] - 185s 1s/step - loss: 0.1819 - accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "trained = model.fit(train_dataset, batch_size=BATCH_SIZE, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132f492",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0b9d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 47s 1s/step - loss: 0.1236 - accuracy: 0.9501\n",
      "Validation loss:     0.12\n",
      "Validation accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "loss1, accuracy1 = model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Validation loss:     {loss1:.2f}\")\n",
    "print(f\"Validation accuracy: {accuracy1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfe84ae0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fe8ac8d2b797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "\n",
    "acc = trained.history['accuracy']\n",
    "val_acc = trained.history['val_accuracy']\n",
    "\n",
    "loss = trained.history['loss']\n",
    "val_loss = trained.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36110948",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60744317",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
